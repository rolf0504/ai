# word2vec

## 來源

* https://radimrehurek.com/gensim/models/word2vec.html
* https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/

## 執行

```
PS D:\ccc\course\ai\python\10-deepLearning\word2vec> python nltk1.py
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\user\AppData\Roaming\nltk_data...
[nltk_data]   Unzipping tokenizers\punkt.zip.
PS D:\ccc\course\ai\python\10-deepLearning\word2vec> python word2vec.py
Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.9968129
Cosine similarity between 'alice' and 'machines' - CBOW :  0.9769483
Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.96269125
Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.9606565
```

## 參考

* [[自然語言處理] #2 Word to Vector 實作教學 (實作篇)](https://medium.com/royes-researchcraft/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-2-word-to-vector-%E5%AF%A6%E4%BD%9C%E6%95%99%E5%AD%B8-%E5%AF%A6%E4%BD%9C%E7%AF%87-e2c1be2346fc)
